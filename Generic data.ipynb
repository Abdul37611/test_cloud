{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2717dc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, NamedTuple, Tuple\n",
    "\n",
    "import googlemaps\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from vrp.config import API_KEY\n",
    "\n",
    "STORES = {\n",
    "    '3165': '5400 Rue Jean-talon O, Montreal, QC, H4P 2T5',\n",
    "    '3174': '670 Applewood Crescent, Vaughan, ON, L4K 4B4',\n",
    "    '1004': '2525 St Clair Ave W, Toronto, ON, M6N 4Z5',\n",
    "    '1056': '680 Laval Dr, Oshawa, ON, L1J 0B5',\n",
    "    '1001': '1525 Bristol Road West Mississauga ON L5V 2P3',\n",
    "    '2892': '17 Tannery Street Mississauga ON L5M 4Z0'\n",
    "}\n",
    "\n",
    "\n",
    "class Vehicle(NamedTuple):\n",
    "    name: str\n",
    "    store: int\n",
    "    speed: float\n",
    "    shift_start: int\n",
    "    shift_end: int\n",
    "    slots_served: List[int]\n",
    "\n",
    "\n",
    "def read_data(\n",
    "    data_file='dataset.xlsx',\n",
    "    config_file='config.xlsx'\n",
    ") -> Tuple[pd.DataFrame, Dict]:\n",
    "\n",
    "    data = pd.read_excel(\n",
    "        data_file,\n",
    "        sheet_name='Export Worksheet',\n",
    "        usecols=[\n",
    "            'ORDER_NO',\n",
    "            'ADDRESS_LINE1',\n",
    "            'ADDRESS_LINE2',\n",
    "            'CITY',\n",
    "            'ZIP_CODE',\n",
    "            'EXTN_MTEP_SHIP_NODE',\n",
    "            'EXTN_MTEP_DEL_START_DATE_TIME',\n",
    "            'EXTN_MTEP_DEL_END_DATE_TIME',\n",
    "            'EXTN_MTEP_DISPENSE_TYPE',\n",
    "            'EXTN_MTEP_VAN_ID'\n",
    "        ],\n",
    "        parse_dates=[\n",
    "            'EXTN_MTEP_DEL_START_DATE_TIME',\n",
    "            'EXTN_MTEP_DEL_END_DATE_TIME'\n",
    "        ],\n",
    "        dtype={\n",
    "            'EXTN_MTEP_SHIP_NODE': str\n",
    "        }\n",
    "    ,engine='openpyxl').rename(\n",
    "        columns={\n",
    "            'ORDER_NO': 'order_id',\n",
    "            'ADDRESS_LINE1': 'address',\n",
    "            'ADDRESS_LINE2': 'address_line2',\n",
    "            'CITY': 'city',\n",
    "            'ZIP_CODE': 'zip',\n",
    "            'EXTN_MTEP_SHIP_NODE': 'store',\n",
    "            'EXTN_MTEP_DEL_START_DATE_TIME': 'start_datetime',\n",
    "            'EXTN_MTEP_DEL_END_DATE_TIME': 'end_datetime',\n",
    "            'EXTN_MTEP_DISPENSE_TYPE': 'dispense_type',\n",
    "            'EXTN_MTEP_VAN_ID': 'van_id'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    conf_df = pd.read_excel(\n",
    "        config_file,\n",
    "        sheet_name='Vehicles&Shift',\n",
    "        usecols=[\n",
    "            'Vehicle',\n",
    "            'Store',\n",
    "            'Vehicle Speed',\n",
    "            'Shift Start',\n",
    "            'Shift End',\n",
    "            'Break Start',\n",
    "            'Break End',\n",
    "            'Slots Served',\n",
    "            'Days Active',\n",
    "            'Distance Threshold (all orders exceeding this travel distance from the store MUST be taken by a truck)'\n",
    "        ],\n",
    "        dtype={\n",
    "            'Store': str,\n",
    "            'Shift Start': str,\n",
    "            'Shift End': str,\n",
    "            'Break Start': str,\n",
    "            'Break End': str,\n",
    "        }\n",
    "    ,engine='openpyxl').rename(columns={\n",
    "        'Distance Threshold (all orders exceeding this travel distance from the store MUST be taken by a truck)': 'Distance Threshold'\n",
    "    })\n",
    "\n",
    "    conf_df['Vehicle'] = conf_df['Vehicle'] + ' ' + conf_df['Shift Start']\n",
    "    \n",
    "    conf_df = conf_df.dropna(axis = 0, how = 'all')\n",
    "\n",
    "    config = {}\n",
    "    for store in pd.unique(conf_df['Store']):\n",
    "        vehicles: Dict[str, Vehicle] = {}\n",
    "        store_df = conf_df.loc[conf_df['Store'] == store]\n",
    "        for _, row in store_df.iterrows():\n",
    "            name = row['Vehicle'][:-5]\n",
    "            speed = row['Vehicle Speed']\n",
    "            shift_start = int(row['Shift Start'][:-2])\n",
    "            shift_end = int(row['Shift End'][:-2])\n",
    "            slots_served = [\n",
    "                int(slot.split('-')[0])\n",
    "                for slot in row['Slots Served'].split(',')\n",
    "            ]\n",
    "            vehicles[row['Vehicle']] = Vehicle(\n",
    "                name, store, speed, shift_start, shift_end, slots_served)\n",
    "        threshold = int(store_df.iloc[0]['Distance Threshold'][:-2])\n",
    "\n",
    "        config[store] = {\n",
    "            'vehicles': vehicles,\n",
    "            'distance_threshold': threshold\n",
    "        }\n",
    "\n",
    "    return data, config\n",
    "\n",
    "\n",
    "def select_data(\n",
    "    data: pd.DataFrame,\n",
    "    config: Dict,\n",
    "    date: str,\n",
    "    store: str\n",
    ") -> Tuple[pd.DataFrame, Dict]:\n",
    "    \"\"\"Select data for one store at a particular date\n",
    "\n",
    "    The date should be in the form of '2022-02-01', and the store should be one\n",
    "    of 3165, 3174, 1004, 1056.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    meta = {}\n",
    "\n",
    "    # Select data from the requested date\n",
    "    next_day = (datetime.strptime(date, '%Y-%m-%d') + timedelta(1))\n",
    "    next_day_str = next_day.strftime('%Y-%m-%d')\n",
    "    data = data.loc[\n",
    "        (data['start_datetime'] > date) &\n",
    "        (data['start_datetime'] < next_day_str)\n",
    "    ]\n",
    "\n",
    "    # Filter data by store\n",
    "    data = data.loc[data['store'] == store]\n",
    "    store_address = STORES[store]\n",
    "\n",
    "    # Process time windows\n",
    "    data['tw_start'] = data['start_datetime'].apply(lambda x: x.hour)\n",
    "    data['tw_end'] = data['end_datetime'].apply(lambda x: x.hour)\n",
    "\n",
    "    # Vehicles\n",
    "    vans = config[store]['vehicles']\n",
    "\n",
    "    # Save info\n",
    "    meta['vans'] = vans\n",
    "    meta['distance_threshold'] = config[store]['distance_threshold']\n",
    "    meta['store_address'] = store_address\n",
    "    meta['store_id'] = store\n",
    "\n",
    "    gmaps = googlemaps.Client(key=API_KEY)\n",
    "\n",
    "    # Clean up address fields\n",
    "    data['city'] = data['city'].apply(lambda x: x.upper().strip())\n",
    "    data['zip'] = data['zip'].str[:3] + ' ' + data['zip'].str[-3:]\n",
    "    addresses = data['address'] + ', ' + data['city'] + ' ' + data['zip']\n",
    "\n",
    "    # Find addresses on Google Map\n",
    "    store_address_id = gmaps.find_place(\n",
    "        store_address, 'textquery')['candidates'][0]['place_id']\n",
    "\n",
    "    if store == \"1001\":\n",
    "        store_address_id= 'ChIJg6KoFqlBK4gREonlbBcdKGk'\n",
    "    elif store == \"2892\":\n",
    "        store_address_id= 'ChIJ5xzrH7lBK4gRmeDvfmAcKNY'\n",
    "    else:\n",
    "        store_address_id = gmaps.find_place(store_address, 'textquery')['candidates'][0]['place_id']\n",
    "\n",
    "    \n",
    "    res = addresses.apply(lambda x: gmaps.find_place(\n",
    "        x, 'textquery')).apply(pd.Series)\n",
    "\n",
    "    # Check validity of addresses\n",
    "    errors = data[res['status']=='ZERO_RESULTS']\n",
    "    address_error=[]\n",
    "    for line, err in errors.iterrows():\n",
    "        address = err['address']\n",
    "        address_error.append(f'Address \"{address}\" on line {line} not recognized! Please fix '\n",
    "            'the address in the data set, remember to remove additional text '\n",
    "            'such as buzzer codes as it is not recognized by Google Maps.')\n",
    "    \n",
    "    if len(address_error)!=0:\n",
    "        return address_error\n",
    "        \n",
    "    data['address_id'] = res['candidates'].apply(lambda x: x[0]['place_id'])\n",
    "\n",
    "    # Clean up address entries in table\n",
    "    address_info = data.address_id.apply(\n",
    "        lambda _id: gmaps.place(_id)['result'])\n",
    "    data['full_address'] = address_info.apply(\n",
    "        lambda info: info['formatted_address'])\n",
    "    data['longitude'] = address_info.apply(\n",
    "        lambda info: info['geometry']['location']['lng'])\n",
    "    data['latitude'] = address_info.apply(\n",
    "        lambda info: info['geometry']['location']['lat'])\n",
    "\n",
    "    # Save store info\n",
    "    meta['store_address_id'] = store_address_id\n",
    "    store_info = gmaps.place(store_address_id)['result']\n",
    "    meta['store_coor'] = [\n",
    "        store_info['geometry']['location']['lat'],\n",
    "        store_info['geometry']['location']['lng']\n",
    "    ]\n",
    "\n",
    "    data['distance'] = data['address_id'].apply(\n",
    "        lambda x: get_shortest_distance(x, meta['store_address_id'], gmaps))\n",
    "\n",
    "    return data, meta\n",
    "\n",
    "\n",
    "def get_dist_mat(\n",
    "    data: pd.DataFrame,\n",
    "    meta: Dict,\n",
    "    dist_mat_file: str,\n",
    "    dur_mat_file: str\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Find the distance matrix\"\"\"\n",
    "\n",
    "    gmaps = googlemaps.Client(key=API_KEY)\n",
    "\n",
    "    store_address_id = 'place_id:' + meta['store_address_id']\n",
    "    addresses = [store_address_id] + [\n",
    "        'place_id:' + x for x in pd.unique(data.address_id).tolist()]\n",
    "\n",
    "    n_addresses = len(addresses)\n",
    "    dist_mat = pd.DataFrame()\n",
    "    dur_mat = pd.DataFrame()\n",
    "    for i in range(int(np.ceil(n_addresses / 10))):\n",
    "        sub_dist_row = pd.DataFrame()\n",
    "        sub_dur_row = pd.DataFrame()\n",
    "        for j in range(int(np.ceil(n_addresses / 10))):\n",
    "            response = gmaps.distance_matrix(\n",
    "                origins=addresses[i*10:i*10+10],\n",
    "                destinations=addresses[j*10:j*10+10]\n",
    "            )\n",
    "            rows = [row['elements'] for row in response['rows']]\n",
    "            sub_dist_mat = pd.DataFrame([[\n",
    "                int(np.ceil(e['distance']['value'])) for e in row\n",
    "            ] for row in rows])\n",
    "            sub_dur_mat = pd.DataFrame([[\n",
    "                int(np.ceil(e['duration']['value'] / 60)) for e in row\n",
    "            ] for row in rows])  # to the whole minute\n",
    "            sub_dist_row = pd.concat([sub_dist_row, sub_dist_mat], axis=1)\n",
    "            sub_dur_row = pd.concat([sub_dur_row, sub_dur_mat], axis=1)\n",
    "        dur_mat = pd.concat([dur_mat, sub_dur_row], axis=0)\n",
    "        dist_mat = pd.concat([dist_mat, sub_dist_row], axis=0)\n",
    "\n",
    "    address_ids = [x[9:] for x in addresses]\n",
    "    dist_mat.columns = address_ids\n",
    "    dist_mat['index'] = address_ids\n",
    "    dist_mat = dist_mat.set_index('index')\n",
    "    dist_mat.index.name = None\n",
    "    dur_mat.columns = address_ids\n",
    "    dur_mat['index'] = address_ids\n",
    "    dur_mat = dur_mat.set_index('index')\n",
    "    dur_mat.index.name = None\n",
    "\n",
    "    dist_mat.to_csv(dist_mat_file)\n",
    "    dur_mat.to_csv(dur_mat_file)\n",
    "\n",
    "    return dist_mat, dur_mat\n",
    "\n",
    "\n",
    "def save_data(data: pd.DataFrame, meta: Dict, data_file: str, meta_file: str):\n",
    "\n",
    "    data.to_csv(data_file, columns=[\n",
    "        'order_id',\n",
    "        'address',\n",
    "        'address_line2',\n",
    "        'city',\n",
    "        'zip',\n",
    "        'longitude',\n",
    "        'latitude',\n",
    "        'address_id',\n",
    "        'store',\n",
    "        'tw_start',\n",
    "        'tw_end',\n",
    "        'start_datetime',\n",
    "        'end_datetime',\n",
    "        'distance',\n",
    "        'dispense_type',\n",
    "        'van_id']\n",
    "    )\n",
    "\n",
    "    with open(meta_file, 'w') as outfile:\n",
    "        json.dump(json.dumps(meta), outfile)\n",
    "\n",
    "\n",
    "def save_config(config: Dict, filename: str):\n",
    "    with open(filename, 'w') as outfile:\n",
    "        json.dump(json.dumps(config), outfile)\n",
    "\n",
    "\n",
    "def read_processed_data(\n",
    "    data_file: str,\n",
    "    meta_file: str\n",
    ") -> Tuple[pd.DataFrame, Dict]:\n",
    "\n",
    "    data = pd.read_csv(\n",
    "        data_file,\n",
    "        index_col=0,\n",
    "        parse_dates=[\n",
    "            'start_datetime',\n",
    "            'end_datetime'\n",
    "        ],\n",
    "        dtype={\n",
    "            'store': str\n",
    "        }\n",
    "    )\n",
    "\n",
    "    with open(meta_file, 'r') as f:\n",
    "        meta = json.loads(json.load(f))\n",
    "\n",
    "    for vehicle in meta['vans']:\n",
    "        meta['vans'][vehicle] = Vehicle(*meta['vans'][vehicle])\n",
    "\n",
    "    return data, meta\n",
    "\n",
    "\n",
    "def read_config(config_file: str) -> Dict:\n",
    "\n",
    "    with open(config_file, 'r') as f:\n",
    "        config = json.loads(json.load(f))\n",
    "\n",
    "    for store in config:\n",
    "        for vehicle in config[store]['vehicles']:\n",
    "            config[store]['vehicles'][vehicle] = Vehicle(\n",
    "                *config[store]['vehicles'][vehicle])\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_shortest_distance(\n",
    "    address_id: str, \n",
    "    store_address: str, \n",
    "    gmaps\n",
    ") -> int:\n",
    "    result = gmaps.directions(\n",
    "        'place_id:' + store_address, \n",
    "        'place_id:' + address_id, \n",
    "        alternatives=True\n",
    "    )\n",
    "    return min(alt['legs'][0]['distance']['value'] for alt in result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6506f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from statistics import variance\n",
    "from typing import Dict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dimod import BinaryQuadraticModel, ConstrainedQuadraticModel\n",
    "from dwave.system import LeapHybridCQMSampler\n",
    "\n",
    "from vrp.tsp import TSP, LoadBalancer\n",
    "\n",
    "\n",
    "def two_stage_cluster(\n",
    "    data: pd.DataFrame,\n",
    "    dur_mat: pd.DataFrame,\n",
    "    meta: Dict,\n",
    "    balance=''\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    data = data.reset_index(drop=True).set_index('order_id')\n",
    "\n",
    "    # First stage cluster based on pre-routing\n",
    "    data = first_stage_cluster(data, dur_mat, meta, balance)\n",
    "\n",
    "    # Second stage cluster\n",
    "    orders_1 = data[data['cluster'].notnull()]\n",
    "    orders_2 = data[data['cluster'].isna()]\n",
    "    cqm = construct_second_stage_cqm(data, orders_1, orders_2, dur_mat, meta)\n",
    "    sample = LeapHybridCQMSampler().sample_cqm(cqm).filter(\n",
    "        lambda row: row.is_feasible)\n",
    "    assignments = dict(\n",
    "        entry[0] for entry in sample.first.sample.items() if entry[1] == 1)\n",
    "    data['cluster'] = data.index.map(assignments)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def first_stage_cluster(\n",
    "    data: pd.DataFrame,\n",
    "    dur_mat: pd.DataFrame,\n",
    "    meta: Dict,\n",
    "    balance=''\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    remaining = data[data.apply(\n",
    "        lambda row: row['distance'] > meta['distance_threshold'] * 1000\n",
    "        and row['dispense_type'] == 'FLEET_DELIVERY', axis=1)]\n",
    "\n",
    "    if len(remaining) == 0:\n",
    "        data['cluster'] = np.nan\n",
    "        return data\n",
    "\n",
    "    subsets = {}\n",
    "    for name, van in meta['vans'].items():\n",
    "        subsets[name] = remaining[remaining.apply(\n",
    "            lambda row: row['tw_start'] in van.slots_served, axis=1)]\n",
    "    vans = sorted(meta['vans'], key=lambda x: len(subsets[x]))\n",
    "\n",
    "    routes = {}\n",
    "    solver = TSP(remaining, dur_mat, meta)\n",
    "    while vans:\n",
    "\n",
    "        van = vans.pop(0)\n",
    "        subset = subsets[van]\n",
    "\n",
    "        departure_time = meta['vans'][van].shift_start * 60\n",
    "        shift_end = meta['vans'][van].shift_end * 60\n",
    "        result = solver.solve_custom(subset, departure_time, shift_end)\n",
    "\n",
    "        routes[van] = result\n",
    "        orders = [order for order, _ in result[1:]]\n",
    "        remaining = remaining.drop(index=orders)\n",
    "        \n",
    "        subsets = {}\n",
    "        for name in vans:\n",
    "            van = meta['vans'][name]\n",
    "            subsets[name] = remaining[remaining.apply(\n",
    "                lambda row: row['tw_start'] in van.slots_served, axis=1)]\n",
    "        vans.sort(key=lambda x: len(subsets[x]))\n",
    "    \n",
    "    if len(remaining) > 0:\n",
    "        print('Pre-routing was unable to assign routes to all required orders')\n",
    "    \n",
    "    if balance == 'naive':\n",
    "        LoadBalancer(data, dur_mat, meta).naive_balance(routes)\n",
    "    elif balance == 'centroid':\n",
    "        LoadBalancer(data, dur_mat, meta).distance_based_balance(routes)\n",
    "    elif balance == 'time':\n",
    "        LoadBalancer(data, dur_mat, meta).minimize_average_travel_time(routes)\n",
    "\n",
    "    for van, route in routes.items():\n",
    "        orders = [order for order, _ in route[1:]]\n",
    "        data.loc[orders, 'cluster'] = van\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def construct_second_stage_cqm(\n",
    "    data: pd.DataFrame,\n",
    "    orders_1: pd.DataFrame,\n",
    "    orders_2: pd.DataFrame,\n",
    "    dur_mat: pd.DataFrame,\n",
    "    meta: Dict\n",
    ") -> ConstrainedQuadraticModel:\n",
    "\n",
    "    cqm = ConstrainedQuadraticModel()\n",
    "    for _, order in data.iterrows():\n",
    "        cqm.add_discrete([(order.name, van) for van in meta['vans']])\n",
    "\n",
    "    # Add distance metric\n",
    "    objective = BinaryQuadraticModel('BINARY')\n",
    "    for u, v in combinations(data.index, r=2):\n",
    "        add_u, add_v = data.loc[u]['address_id'], data.loc[v]['address_id']\n",
    "        if data.loc[u]['tw_start'] > data.loc[v]['tw_start']:\n",
    "            cost = dur_mat.loc[add_v][add_u]\n",
    "        elif data.loc[u]['tw_start'] < data.loc[v]['tw_start']:\n",
    "            cost = dur_mat.loc[add_u][add_v]\n",
    "        else:\n",
    "            cost = (dur_mat.loc[add_u][add_v] + dur_mat.loc[add_v][add_u]) / 2\n",
    "        for van in meta['vans']:\n",
    "            objective.set_quadratic((u, van), (v, van), cost)\n",
    "    cqm.set_objective(objective)\n",
    "\n",
    "    # Add time window constraints\n",
    "    add_time_window_constraints(cqm, orders_2, meta)\n",
    "\n",
    "    # Points already assigned a cluster\n",
    "    pre = []\n",
    "    for _, order in orders_1.iterrows():\n",
    "        pre.append(((order.name, order['cluster']), 1))\n",
    "    cqm.add_constraint(pre, '==', len(pre))\n",
    "\n",
    "    return cqm\n",
    "\n",
    "\n",
    "def add_time_window_constraints(\n",
    "    cqm: ConstrainedQuadraticModel,\n",
    "    data: pd.DataFrame,\n",
    "    meta: Dict\n",
    "):\n",
    "    for van in meta['vans']:\n",
    "        slots = meta['vans'][van].slots_served\n",
    "        unservable_orders = []\n",
    "        for _, order in data.iterrows():\n",
    "            if order['tw_start'] not in slots:\n",
    "                unservable_orders.append(((order.name, van), 1))\n",
    "        cqm.add_constraint(unservable_orders, '==', 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5119d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, meta = read_processed_data(\n",
    "    'resources_test/1001-2022-10-12/processed_data.csv', \n",
    "    'resources_test/1001-2022-10-12/meta.json'\n",
    ")\n",
    "dist_mat = pd.read_csv(\n",
    "    'resources_test/1001-2022-10-12/distance_matrix.csv',\n",
    "    index_col=0\n",
    ")\n",
    "dur_mat = pd.read_csv(\n",
    "    'resources_test/1001-2022-10-12/duration_matrix.csv',\n",
    "    index_col=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41327e70",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "SampleSet is empty",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/GitHub/venv_dwave/lib/python3.8/site-packages/dimod/sampleset.py:890\u001b[0m, in \u001b[0;36mSampleSet.first\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 890\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m(\u001b[49m\u001b[43msorted_by\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43menergy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSample\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[0;31mStopIteration\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m clustered_data \u001b[38;5;241m=\u001b[39m \u001b[43mtwo_stage_cluster\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdur_mat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [4], line 32\u001b[0m, in \u001b[0;36mtwo_stage_cluster\u001b[0;34m(data, dur_mat, meta, balance)\u001b[0m\n\u001b[1;32m     28\u001b[0m cqm \u001b[38;5;241m=\u001b[39m construct_second_stage_cqm(data, orders_1, orders_2, dur_mat, meta)\n\u001b[1;32m     29\u001b[0m sample \u001b[38;5;241m=\u001b[39m LeapHybridCQMSampler()\u001b[38;5;241m.\u001b[39msample_cqm(cqm)\u001b[38;5;241m.\u001b[39mfilter(\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m row: row\u001b[38;5;241m.\u001b[39mis_feasible)\n\u001b[1;32m     31\u001b[0m assignments \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m---> 32\u001b[0m     entry[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m \u001b[43msample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfirst\u001b[49m\u001b[38;5;241m.\u001b[39msample\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m entry[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     33\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mmap(assignments)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/Documents/GitHub/venv_dwave/lib/python3.8/site-packages/dimod/sampleset.py:892\u001b[0m, in \u001b[0;36mSampleSet.first\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata(sorted_by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menergy\u001b[39m\u001b[38;5;124m'\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSample\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m--> 892\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m is empty\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m))\n",
      "\u001b[0;31mValueError\u001b[0m: SampleSet is empty"
     ]
    }
   ],
   "source": [
    "clustered_data = two_stage_cluster(data, dur_mat, meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94cf8cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
